{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **Importing Libraries**"
      ],
      "metadata": {
        "id": "dwS9Z1PUicTW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfd-gTTHbrkS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Loading dataset into train and test variables**"
      ],
      "metadata": {
        "id": "35vHISX2ihpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "iHzChQWdgQbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d19e30-3e5b-4120-d820-75a316bfb982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Data Preprocessing**"
      ],
      "metadata": {
        "id": "iu7Vc6Z3inka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Normalizing pixels value to be between 0 and 1**"
      ],
      "metadata": {
        "id": "S801d6STitPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "5A-LdKBcgSkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Converting class labels to one-hot encoded format**"
      ],
      "metadata": {
        "id": "BojgnvWcivYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "pEKfCMw2gUkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Data augmentation to improve generalization**"
      ],
      "metadata": {
        "id": "yeyWOPrJiwNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True)\n",
        "datagen.fit(x_train)"
      ],
      "metadata": {
        "id": "7Fw4CDWbgWVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2/3. Network Architecture Design and Actiavtion Function**"
      ],
      "metadata": {
        "id": "o2q_-W9Vgfqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Designing the neural network**"
      ],
      "metadata": {
        "id": "cVSWB2BijICD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()"
      ],
      "metadata": {
        "id": "UTRIWJLNgbkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Flattening layer to convert 32x32x3 image to 3072 (32*32*3)**"
      ],
      "metadata": {
        "id": "r7agUT1ZjOvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Flatten(input_shape=(32, 32, 3)))"
      ],
      "metadata": {
        "id": "ydWL0LM9gelU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8cdfda-9fb5-4be1-8b97-ef67cdeb4a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hidden layers**"
      ],
      "metadata": {
        "id": "tuBSqj5PjUL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))"
      ],
      "metadata": {
        "id": "lhtyg5mIghgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Dropout to prevent overfitting"
      ],
      "metadata": {
        "id": "5w08HcVrjaBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Output layer with softmax activation for multi-class classification**"
      ],
      "metadata": {
        "id": "FKbfaEpvjbaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "hKrUoy7igjUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Summary of the model architecture**"
      ],
      "metadata": {
        "id": "gBs6fHP2jjw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "c0-GdkX-gk-o",
        "outputId": "3b87c6b6-d25a-4ad6-81da-4b723dfa0a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,573,376\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m2,570\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,376</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,707,274\u001b[0m (6.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,707,274</span> (6.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,707,274\u001b[0m (6.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,707,274</span> (6.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Justify your choice of network architecture, including the number of layers, types of layers, and the number of neurons/filters in each layer.**\n",
        "\n",
        "Input Layer (Flatten): The CIFAR-10 images are 32x32x3, so the flatten layer converts this into a 3072-dimensional vector, making it suitable for fully connected layers.\n",
        "\n",
        "Hidden Layer 1 (512 neurons, ReLU): A large number of neurons (512) allows the model to capture complex patterns, while ReLU helps avoid vanishing gradients and speeds up convergence.\n",
        "\n",
        "Dropout (0.5): Added after the first hidden layer to prevent overfitting by randomly turning off 50% of the neurons during training.\n",
        "\n",
        "Hidden Layer 2 (256 neurons, ReLU): A smaller layer than the first to reduce dimensionality and complexity, while still maintaining the network’s ability to learn patterns.\n",
        "\n",
        "Output Layer (10 neurons, Softmax): Softmax activation provides class probabilities for the 10 categories, which is standard for multi-class classification."
      ],
      "metadata": {
        "id": "mcx4ftaDglkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain why you selected a particular activation function and its role in\n",
        "the backpropagation process.**\n",
        "\n",
        "I selected ReLU for hidden layers because it helps prevent the vanishing gradient problem and allows faster convergence during backpropagation by only passing positive values.\n",
        "\n",
        "For the output layer, Softmax is used to normalize the output into probabilities for multi-class classification, aiding in error calculation during backpropagation."
      ],
      "metadata": {
        "id": "ZoY7vhn80BqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. Loss Function and Optimizer**"
      ],
      "metadata": {
        "id": "S0hPtMXEgomm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Compile the model**"
      ],
      "metadata": {
        "id": "GIdmQppg0NQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rU-3PZ1Ygo8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain how the learning rate affects the backpropagation process.**\n",
        "\n",
        "The learning rate controls the step size at which the model updates its weights during backpropagation. A high learning rate can lead to overshooting the optimal solution, causing instability, while a low learning rate results in slow convergence, requiring more epochs to reach the minimum of the loss function. Optimal learning rate ensures balanced and efficient convergence."
      ],
      "metadata": {
        "id": "QnUqJ58m0ZIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Training the model**"
      ],
      "metadata": {
        "id": "fNufI0Oz0Rwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=64), epochs=50, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlPgCnvZgqqc",
        "outputId": "5144c562-ac0d-4d61-f9e6-8d02ee748f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 114ms/step - accuracy: 0.1872 - loss: 2.2228 - val_accuracy: 0.2801 - val_loss: 1.9721\n",
            "Epoch 2/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 88ms/step - accuracy: 0.2260 - loss: 2.0516 - val_accuracy: 0.2973 - val_loss: 1.9531\n",
            "Epoch 3/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 87ms/step - accuracy: 0.2352 - loss: 2.0232 - val_accuracy: 0.2894 - val_loss: 1.9796\n",
            "Epoch 4/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 86ms/step - accuracy: 0.2503 - loss: 1.9985 - val_accuracy: 0.3296 - val_loss: 1.9473\n",
            "Epoch 5/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 85ms/step - accuracy: 0.2586 - loss: 1.9826 - val_accuracy: 0.3148 - val_loss: 1.9287\n",
            "Epoch 6/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 84ms/step - accuracy: 0.2602 - loss: 1.9703 - val_accuracy: 0.3212 - val_loss: 1.9338\n",
            "Epoch 7/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 89ms/step - accuracy: 0.2598 - loss: 1.9749 - val_accuracy: 0.3485 - val_loss: 1.9068\n",
            "Epoch 8/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 87ms/step - accuracy: 0.2707 - loss: 1.9545 - val_accuracy: 0.3441 - val_loss: 1.9214\n",
            "Epoch 9/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 84ms/step - accuracy: 0.2713 - loss: 1.9523 - val_accuracy: 0.3111 - val_loss: 1.9183\n",
            "Epoch 10/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 86ms/step - accuracy: 0.2824 - loss: 1.9309 - val_accuracy: 0.3297 - val_loss: 1.9348\n",
            "Epoch 11/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 85ms/step - accuracy: 0.2851 - loss: 1.9287 - val_accuracy: 0.3533 - val_loss: 1.8877\n",
            "Epoch 12/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 84ms/step - accuracy: 0.2748 - loss: 1.9486 - val_accuracy: 0.3400 - val_loss: 1.8889\n",
            "Epoch 13/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 84ms/step - accuracy: 0.2845 - loss: 1.9353 - val_accuracy: 0.3299 - val_loss: 1.9099\n",
            "Epoch 14/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 85ms/step - accuracy: 0.2893 - loss: 1.9270 - val_accuracy: 0.3316 - val_loss: 1.8962\n",
            "Epoch 15/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 88ms/step - accuracy: 0.2918 - loss: 1.9197 - val_accuracy: 0.3413 - val_loss: 1.8869\n",
            "Epoch 16/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 89ms/step - accuracy: 0.2906 - loss: 1.9145 - val_accuracy: 0.3057 - val_loss: 1.9209\n",
            "Epoch 17/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 85ms/step - accuracy: 0.2934 - loss: 1.9135 - val_accuracy: 0.3060 - val_loss: 1.9254\n",
            "Epoch 18/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 84ms/step - accuracy: 0.2924 - loss: 1.9057 - val_accuracy: 0.3549 - val_loss: 1.9039\n",
            "Epoch 19/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 86ms/step - accuracy: 0.2948 - loss: 1.9139 - val_accuracy: 0.3355 - val_loss: 1.8772\n",
            "Epoch 20/50\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 90ms/step - accuracy: 0.2950 - loss: 1.9165 - val_accuracy: 0.3596 - val_loss: 1.8709\n",
            "Epoch 21/50\n",
            "\u001b[1m134/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 84ms/step - accuracy: 0.2978 - loss: 1.8979"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does the choice of optimizer and learning rate influence the convergence of the network? How would you adjust the learning rate if the model is not converging properly?**\n",
        "\n",
        "The choice of optimizer and learning rate directly affects how quickly and smoothly the model converges to the optimal solution. Optimizers like Adam adapt the learning rate dynamically for each parameter, improving convergence speed and stability, while SGD uses a fixed learning rate, often requiring more tuning.\n",
        "\n",
        "If the model is not converging (e.g., loss oscillates or plateaus), the learning rate can be adjusted. You can reduce the learning rate to ensure smaller, more stable weight updates, or implement learning rate scheduling to gradually lower it during training for smoother convergence. If the rate is too low and learning is very slow, increase it slightly to accelerate progress."
      ],
      "metadata": {
        "id": "gf5vvuXP0oEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does backpropagation update the weights in each layer, and what role does the learning rate play in this process?**\n",
        "\n",
        "Backpropagation updates the weights in each layer by calculating the gradient of the loss function with respect to each weight through the chain rule. It propagates errors backward from the output to the input layer, adjusting weights to minimize the loss. The learning rate controls the size of these updates: a smaller rate makes the weight adjustments smaller and more precise, while a larger rate results in bigger updates, which may either speed up convergence or cause instability if too large."
      ],
      "metadata": {
        "id": "ZYDI3J7EFCS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Model Evaluation**"
      ],
      "metadata": {
        "id": "f2KxKprNgwq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Evaluate the model**"
      ],
      "metadata": {
        "id": "QqQO_oej09wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "-RDf7g84gxEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "\n",
        "At 10 Epochs: The model's test accuracy of 0.0815 (or 8.15%) and loss of 2.4368 indicate that the model is performing poorly on the CIFAR-10 dataset. With 10 classes, random guessing would give an accuracy of around 10%, so achieving 8.15% accuracy means the model is barely better than random, if at all.\n",
        "\n",
        "At 50 Epochs: The model's overall accuracy on the test set is 36%, which indicates it's only correctly classifying the test images about one-third of the time. This is quite low for a dataset like CIFAR-10, suggesting the model may need improvements."
      ],
      "metadata": {
        "id": "_HwjJraHFeWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Predict labels**"
      ],
      "metadata": {
        "id": "D7W-7NG51BAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)"
      ],
      "metadata": {
        "id": "CybEMKu4g1MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Confusion Matrix**"
      ],
      "metadata": {
        "id": "i6IqBgV_1F1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)"
      ],
      "metadata": {
        "id": "VhyDHUSXg3EV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Plot Confusion Matrix**"
      ],
      "metadata": {
        "id": "IftMArkA1KSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BVk56OLSg4xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "\n",
        "at 10 epochs: The confusion matrix illustrates that the model is struggling to classify the CIFAR-10 images accurately. Ideally, correct predictions would be along the diagonal of the matrix, but in this case, misclassifications are widespread across many classes. For example, class 0 (airplanes) is frequently confused with class 3 (cats) and class 5 (dogs), indicating that the model finds it difficult to differentiate between visually similar categories. Additionally, class 6 (frogs) shows a relatively better performance, with 562 correct predictions, but other classes such as 2 (birds) and 3 (cats) have significant misclassifications, often being predicted as other categories.\n",
        "\n",
        "at 50 epochs: he confusion matrix displayed indicates that the model is having difficulty in accurately distinguishing between different classes. Diagonal elements represent correct classifications, while off-diagonal elements indicate misclassifications. For instance, class 0 (airplanes) is correctly predicted 299 times but is frequently confused with class 3 (cats) and class 5 (dogs). Similarly, class 1 (automobiles) has 429 correct predictions but is often misclassified as class 3 or class 8. Class 6 (frogs) shows a relatively better performance with 562 correct predictions, suggesting that the model is more effective at identifying this class compared to others."
      ],
      "metadata": {
        "id": "s2EgYXfCWm5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Classification Report**"
      ],
      "metadata": {
        "id": "8wcrikWJ1Rqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred_classes))"
      ],
      "metadata": {
        "id": "Wyn_ivgkg6jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "\n",
        "At 50 Ephocs: The classification report indicates that the model's overall performance is poor, with an accuracy of only 36%. The model struggles particularly with certain classes like birds, dogs, and cats, where both precision and recall are extremely low. The low recall across many classes suggests the model is failing to identify a significant number of true instances, leading to underperformance. Additionally, the F1-scores reveal an imbalance between precision and recall, indicating the model is not accurately classifying images and also missing many correct predictions. This suggests the need for a more robust architecture, likely a convolutional neural network (CNN), better data preprocessing, and additional training to improve generalization and overall accuracy."
      ],
      "metadata": {
        "id": "Sa07PpxvWRcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How can you further improve model performance if the accuracy is low?**\n",
        "\n",
        "To improve model performance, switch to Convolutional Neural Networks (CNNs), as they are more suited for image classification tasks. Implement data augmentation and regularization techniques like Dropout and Batch Normalization to reduce overfitting and stabilize training. Increase the number of training epochs and use learning rate scheduling or adaptive optimizers like Adam for better convergence. Additionally, explore hyperparameter tuning and consider transfer learning with pretrained models to boost accuracy further."
      ],
      "metadata": {
        "id": "fAQrIW-ULgX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Optimization Strategies**"
      ],
      "metadata": {
        "id": "fB9ybl2gg8IJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define early stopping and learning rate scheduler**"
      ],
      "metadata": {
        "id": "dFz_dLwg1ahE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)"
      ],
      "metadata": {
        "id": "RQf3VBRjg8cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Train the model with early stopping and learning rate scheduling**"
      ],
      "metadata": {
        "id": "qvmM-gWX1e1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                    epochs=25,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[early_stopping, lr_scheduler])"
      ],
      "metadata": {
        "id": "7uYwbWkyhCSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is weight initialization important, and how does it impact the\n",
        "convergence of your network?**\n",
        "\n",
        "Weight initialization is important to prevent **vanishing or exploding gradients**, which can slow or destabilize training. Proper initialization ensures faster and more stable **convergence** by starting the model closer to optimal solutions. It also **breaks symmetry** between neurons, allowing them to learn distinct features. Poor initialization can cause the network to struggle in learning patterns, leading to slow or ineffective training. Overall, it helps the network train more efficiently and effectively."
      ],
      "metadata": {
        "id": "sDeZvIcpL2D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "4fmNR6oaNl_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieved a test accuracy of approximately 38.3% after implementing early stopping and a learning rate scheduler. While this is an improvement compared to earlier results, the overall accuracy is still relatively low. This suggests that while early stopping and learning rate adjustments have helped stabilize the learning process and likely prevented overfitting, the model's architecture may still not be sufficiently complex or effective for the CIFAR-10 dataset."
      ],
      "metadata": {
        "id": "Vk0XoS1seqWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fbi5jBmMNogW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}